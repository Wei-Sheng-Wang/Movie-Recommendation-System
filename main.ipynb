{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6s9Gf3GDtuAV",
   "metadata": {
    "id": "6s9Gf3GDtuAV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/content/drive/MyDrive/Colab Notebooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9545658c",
   "metadata": {
    "id": "9545658c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random \n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# import import_ipynb\n",
    "# from importlib import reload\n",
    "%run MLP.ipynb\n",
    "%run GMF.ipynb\n",
    "%run Dataset.ipynb\n",
    "%run NCF.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926d5326",
   "metadata": {
    "id": "926d5326"
   },
   "outputs": [],
   "source": [
    "def train_test_split(ratings_df):\n",
    "    # rank ratings of each user according to time in descending order, i.e. latest ratings have rank 1\n",
    "    ratings_df['rank_time'] = ratings_df.groupby(['userId'])['timestamp'].rank(method='first', ascending=False)\n",
    "    # create training and testing set\n",
    "    test_set = ratings_df[ratings_df['rank_time'] == 1]\n",
    "    train_set = ratings_df[ratings_df['rank_time'] > 1] \n",
    "\n",
    "    train_set, test_set = train_set[['userId', 'movieId', 'rating']], test_set[['userId', 'movieId', 'rating']]\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "\n",
    "def get_negative_instances(ratings_df) -> pd.DataFrame:\n",
    "\n",
    "    uniq_movies = set(ratings_df['movieId'].unique())\n",
    "\n",
    "    user_item_interaction = ratings_df.groupby('userId')['movieId'].apply(set).reset_index().rename(\n",
    "                columns = {'movieId':'interacted_items'})\n",
    "    \n",
    "    user_item_interaction['negative_instances'] = user_item_interaction['interacted_items'].apply(lambda positive_instances: [x for x in uniq_movies if x not in positive_instances])\n",
    "    \n",
    "    return user_item_interaction[['userId', 'negative_instances']]\n",
    "\n",
    "\n",
    "    \n",
    "# sample negative instances\n",
    "def sample_neg(negative_instances_df, num_neg_samples):  \n",
    "    negative_instances_df['negative_samples'] = negative_instances_df['negative_instances'].apply(lambda x: random.sample(x, num_neg_samples))\n",
    "    \n",
    "    return negative_instances_df[['userId', 'negative_samples']]\n",
    "\n",
    "\n",
    "\n",
    "# train_df columns = {userId, movieId, rating}\n",
    "# negative_instances_df columns = {userId, negative_instances}\n",
    "# return Dataset object\n",
    "def get_dataset(df, negative_instances_df, num_neg_samples=4):\n",
    " \n",
    "    # sample 4 negative instance per positive instance,\n",
    "    # columns = {userId, negative_samples}\n",
    "    neg_samples_df = sample_neg(negative_instances_df, num_neg_samples)\n",
    "    # merge dataframes to include movies and ratings\n",
    "    neg_samples_df = pd.merge(df, neg_samples_df, on='userId')\n",
    "\n",
    "    users, movies, labels = [], [], []\n",
    "    for row in neg_samples_df.itertuples():\n",
    "        users.append(row.userId)\n",
    "        movies.append(row.movieId)\n",
    "        labels.append(row.rating)\n",
    "        for i in range(num_neg_samples):\n",
    "            users.append(row.userId)\n",
    "            movies.append(row.negative_samples[i])\n",
    "            labels.append(0)\n",
    "\n",
    "    # create custom Dataset \n",
    "    return RatingsDataset(users, movies, labels)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# # actual_ratings_df contains the latest ratings of each user for a moviea\n",
    "# # hundred_neg_samples_df is a dataframe with columns = {userId, negative_samples}, where negative_samples is a list of 100 negative instances of each user\n",
    "def evaluate(users, movies, predictions, test_df, topk=10):\n",
    "    # _, indices = torch.topk(predictions, topk)\n",
    "    # n = len(test_df)\n",
    "    # topk_users, topk_movies = [users[i] for i in indices], [movies[i] for i in indices]\n",
    "    # pred_df = test_df.loc[test_df['userId'].isin(topk_users) and test_df['movieId'].isin(topk_movies)]\n",
    "\n",
    "    # # hit ratio\n",
    "    # hit_ratio = len(pred_df) / n \n",
    "    # # NCDG \n",
    "    # pred_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    pred_df = pd.DataFrame(data={'userId':users.cpu(), 'pred_movieId':movies.cpu(), 'pred_rating':predictions.cpu()})\n",
    "    n = len(test_df)\n",
    "    pred_df.loc[:,'userId'] = pred_df.userId.astype('int')\n",
    "    pred_df.loc[:,'pred_movieId'] = pred_df.pred_movieId.astype('int')\n",
    "    pred_df.loc[:, 'pred_rating'] = pred_df.pred_rating.astype('float')\n",
    "\n",
    "    pred_df.loc[:,'rank'] = pred_df.groupby(['userId'])['pred_rating'].rank(method='first', ascending=False)\n",
    "    pred_df = pred_df.loc[pred_df['rank'] <= topk]\n",
    "    \n",
    "    \n",
    "    pred_ground_df = pd.merge(pred_df, test_df, on='userId')\n",
    "\n",
    "    pred_ground_df = pred_ground_df.loc[pred_ground_df['pred_movieId'] == pred_ground_df['movieId']].copy()\n",
    "    # hit ratio\n",
    "    hit_ratio = len(pred_ground_df) / n\n",
    "\n",
    "    # NCDG\n",
    "    # since each user rates item in the test set, we normalized NCDG by dividing DCG by the total number of unique users\n",
    "    pred_ground_df['ncdg'] = pred_ground_df['rank'].apply(lambda rank: 1.0 / np.log2(rank + 1))\n",
    "    ncdg = pred_ground_df.loc[:,'ncdg'].sum() / n\n",
    "\n",
    "\n",
    "    return hit_ratio, ncdg\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3bd0ce",
   "metadata": {
    "id": "2f3bd0ce"
   },
   "outputs": [],
   "source": [
    "def train(model, train_df, test_df, negative_instances, epochs, batch_size, lr, path):\n",
    "    device = torch.device('cuda')\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        if e == 0:\n",
    "            lr = 0.001\n",
    "        else:\n",
    "            lr = 0.0000001\n",
    "        # get training Dataset\n",
    "        train_dataset = get_dataset(train_df, negative_instances, 4)\n",
    "        # create train loader\n",
    "        train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "        test_dataset = get_dataset(test_df, negative_instances_df, 99)\n",
    "        test_size = len(test_dataset.users)\n",
    "\n",
    "        test_loader = DataLoader(test_dataset, test_size, shuffle=False)\n",
    "\n",
    "        running_loss = 0 \n",
    "        best_loss = 1000\n",
    "        # set model to training mode \n",
    "        model.train()\n",
    "        for users, movies, ratings, in train_loader:\n",
    "            users, movies, ratings = users.to(device), movies.to(device), ratings.to(device)\n",
    "            # zero gradient \n",
    "            optimizer.zero_grad()\n",
    "            # calculate output \n",
    "            output = model.forward(users, movies)\n",
    "            # convert to float\n",
    "            ratings = ratings.float()\n",
    "            # caulcate loss \n",
    "            loss = criterion(output, ratings.unsqueeze(1))\n",
    "            # calculate gradient \n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # avg_hit_ratio, avg_ncdg = 0, 0\n",
    "        else:\n",
    "            # set to evaluation mode (turn off dropout)\n",
    "            model.eval()\n",
    "            best_hit_ratio, best_ncdg = 0, 0\n",
    "            # turn off gradient\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for users, movies, _ in test_loader:\n",
    "                    users, movies = users.to(device), movies.to(device)\n",
    "                    # calculate predictions\n",
    "                    output = model.forward(users, movies)\n",
    "                    hit_ratio, ncdg = evaluate(users, movies, output.view(-1), test_df)\n",
    "\n",
    "        if hit_ratio > best_hit_ratio and ncdg > best_ncdg :\n",
    "            torch.save(model.state_dict(), path)\n",
    "            best_hit_ratio = hit_ratio\n",
    "            best_ncdg = ncdg \n",
    "\n",
    "        print(f'iteration {e}: loss per epoch: {running_loss/len(train_loader)}, hit_ratio: {hit_ratio},  NCDG: {ncdg}')\n",
    "\n",
    "    \n",
    "    return model \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VnhBt5PcACNr",
   "metadata": {
    "id": "VnhBt5PcACNr"
   },
   "outputs": [],
   "source": [
    "# read csv files \n",
    "ratings_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ratings_1m.csv')\n",
    "\n",
    "# convert ratings to binary\n",
    "ratings_df.loc[:,'rating'] = 1\n",
    "ratings_df.userId = ratings_df.userId.astype(int)\n",
    "ratings_df.movieId = ratings_df.movieId.astype(int)\n",
    "\n",
    "# map users \n",
    "movieId_mapping = {val: i for i, val in enumerate(ratings_df['movieId'].unique())}\n",
    "ratings_df['movieId'] = ratings_df['movieId'].map(movieId_mapping)\n",
    "\n",
    "# number of unique users and movies, used for embedding \n",
    "num_uniq_users = ratings_df['userId'].nunique() + 1\n",
    "num_uniq_movies = ratings_df['movieId'].nunique() + 1\n",
    "\n",
    "\n",
    "# columns = {userId, negative_instances}\n",
    "# sample from full dataset\n",
    "negative_instances_df = get_negative_instances(ratings_df)\n",
    "\n",
    "# split into training and test data\n",
    "train_df, ground_truth_df = train_test_split(ratings_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a77b2cb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5941213,
     "status": "ok",
     "timestamp": 1641321055532,
     "user": {
      "displayName": "Wei Sheng Wang",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02224445722095081810"
     },
     "user_tz": 0
    },
    "id": "9a77b2cb",
    "outputId": "4a59df6d-f90a-48fe-b0b7-b02459a1532a"
   },
   "outputs": [],
   "source": [
    "# create GMF, MLP and NCF models\n",
    "# mlp_model = MLP(num_users=num_uniq_users + 1, num_items=num_uniq_movies + 1, latent_dim=8)\n",
    "# mlp_model = train(mlp_model, train_df, ground_truth_df, negative_instances_df, 100, 256, 0.0005, path='./MLP.pt')\n",
    "\n",
    "# gmf_model = GMF(num_users=num_uniq_users + 1, num_items=num_uniq_movies + 1, latent_dim=8)\n",
    "# gmf_model = train(gmf_model, train_df, ground_truth_df, negative_instances_df, 100, 512, path='./GMF.pt')\n",
    "\n",
    "ncf_model = NCF(num_users=num_uniq_users+1, num_items=num_uniq_movies+1, latent_dim=8)\n",
    "ncf_model = train(ncf_model, train_df, ground_truth_df, negative_instances_df, 100, 256, lr=0.001, path='NCF.pt')\n",
    "\n",
    "# # save GMF, MLP model\n",
    "# torch.save(gmf_model.state_dict(), './GMF.pt')\n",
    "# torch.save(mlp_model.state_dict(), './MLP.pt')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}